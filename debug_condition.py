#!/usr/bin/env python3
"""
è°ƒè¯•è„šæœ¬ï¼šä¸“é—¨æµ‹è¯•conditionæ˜¯å¦çœŸçš„åœ¨å½±å“æ¨¡å‹
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os
sys.path.append(os.getcwd())

from models.transformer import MotionTransformer

def test_condition_step_by_step():
    """é€æ­¥æµ‹è¯•conditionçš„å½±å“"""
    
    print("=== é€æ­¥è°ƒè¯•conditionå½±å“ ===")
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
    batch_size = 2
    seq_len = 10
    input_feats = 48
    condition_dim = 4
    
    # åˆ›å»ºæç«¯ä¸åŒçš„condition
    condition_data = torch.randn(100, condition_dim)
    condition1 = torch.ones(batch_size, condition_dim) * 5.0   # å¤§æ­£å€¼
    condition2 = torch.ones(batch_size, condition_dim) * -5.0  # å¤§è´Ÿå€¼
    
    print(f"Condition1: {condition1[0]}")
    print(f"Condition2: {condition2[0]}")
    
    # åˆ›å»ºæ¨¡å‹
    model = MotionTransformer(
        input_feats=input_feats,
        num_frames=seq_len,
        num_layers=2,  # å‡å°‘å±‚æ•°ï¼Œç®€åŒ–æµ‹è¯•
        num_heads=2,
        latent_dim=128,  # å‡å°‘ç»´åº¦
        dropout=0.0,  # å…³é—­dropout
        condition=condition_data,
    )
    
    print(f"æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    print(f"Condition encoder: {model.condition_encoder}")
    
    # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    model.train()
    
    # åˆ›å»ºç›¸åŒçš„è¾“å…¥
    x = torch.randn(batch_size, seq_len, input_feats)
    timesteps = torch.ones(batch_size, dtype=torch.long) * 500  # å›ºå®štimestep
    
    print(f"è¾“å…¥x: {x.shape}")
    print(f"Timesteps: {timesteps}")
    
    # æµ‹è¯•condition encoderå•ç‹¬çš„è¾“å‡º
    print("\n=== æµ‹è¯•condition encoder ===")
    with torch.no_grad():
        cond_proj1 = model.condition_encoder(condition1)
        cond_proj2 = model.condition_encoder(condition2)
        
        print(f"Condition projection 1 mean: {torch.mean(cond_proj1):.6f}")
        print(f"Condition projection 2 mean: {torch.mean(cond_proj2):.6f}")
        print(f"Condition projection difference: {torch.mean(torch.abs(cond_proj1 - cond_proj2)):.6f}")
    
    # æµ‹è¯•å®Œæ•´çš„å‰å‘ä¼ æ’­
    print("\n=== æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­ ===")
    
    # å¯ç”¨è°ƒè¯•ä¿¡æ¯
    original_forward = model.forward
    
    def debug_forward(self, x, timesteps, mod=None, condition=None):
        B, T = x.shape[0], x.shape[1]
        
        # Time embedding
        from models.transformer import timestep_embedding
        emb = self.time_embed(timestep_embedding(timesteps, self.latent_dim))
        print(f"Time embedding mean: {torch.mean(emb):.6f}")
        
        # Mod embedding
        if mod is not None:
            mod_proj = self.cond_embed(mod.reshape(B, -1))
            emb = emb + mod_proj
            print(f"After mod, embedding mean: {torch.mean(emb):.6f}")
        
        # Condition embedding
        if condition is not None and self.condition_encoder is not None:
            cond_proj = self.condition_encoder(condition)
            print(f"Condition projection mean: {torch.mean(cond_proj):.6f}")
            condition_weight = 1.0
            emb_before = emb.clone()
            emb = emb + condition_weight * cond_proj
            print(f"Embedding change due to condition: {torch.mean(torch.abs(emb - emb_before)):.6f}")
            print(f"After condition, embedding mean: {torch.mean(emb):.6f}")
        
        # Joint embedding
        h = self.joint_embed(x)
        h = h + self.sequence_embedding.unsqueeze(0)[:, :T, :]
        print(f"Joint embedding mean: {torch.mean(h):.6f}")
        
        # Transformer blocks
        i = 0
        prelist = []
        for module in self.temporal_decoder_blocks:
            if i < (self.num_layers // 2):
                prelist.append(h)
                h = module(h, emb)
            elif i >= (self.num_layers // 2):
                h = module(h, emb)
                h += prelist[-1]
                prelist.pop()
            i += 1
        
        print(f"After transformer blocks mean: {torch.mean(h):.6f}")
        
        # Output
        output = self.out(h).view(B, T, -1).contiguous()
        print(f"Final output mean: {torch.mean(output):.6f}")
        
        return output
    
    # æ›¿æ¢forwardæ–¹æ³•è¿›è¡Œè°ƒè¯•
    import types
    model.forward = types.MethodType(debug_forward, model)
    
    print("\n--- æµ‹è¯•condition1 ---")
    with torch.no_grad():
        output1 = model(x, timesteps, condition=condition1)
    
    print("\n--- æµ‹è¯•condition2 ---")
    with torch.no_grad():
        output2 = model(x, timesteps, condition=condition2)
    
    print("\n=== æœ€ç»ˆç»“æœæ¯”è¾ƒ ===")
    diff = torch.mean(torch.abs(output1 - output2)).item()
    print(f"è¾“å‡ºå·®å¼‚: {diff:.6f}")
    
    if diff > 1e-4:
        print("âœ“ ConditionæˆåŠŸå½±å“äº†æ¨¡å‹è¾“å‡ºï¼")
        return True
    else:
        print("âœ— Conditionæ²¡æœ‰æ˜æ˜¾å½±å“æ¨¡å‹è¾“å‡º")
        return False

def test_simple_condition_network():
    """æµ‹è¯•ä¸€ä¸ªç®€åŒ–çš„conditionç½‘ç»œ"""
    print("\n=== æµ‹è¯•ç®€åŒ–çš„conditionç½‘ç»œ ===")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•ç½‘ç»œ
    condition_dim = 4
    embed_dim = 128
    
    condition_encoder = nn.Sequential(
        nn.Linear(condition_dim, embed_dim // 2),
        nn.ReLU(),
        nn.Linear(embed_dim // 2, embed_dim),
    )
    
    # æµ‹è¯•æ•°æ®
    condition1 = torch.ones(1, condition_dim) * 2.0
    condition2 = torch.ones(1, condition_dim) * -2.0
    
    with torch.no_grad():
        out1 = condition_encoder(condition1)
        out2 = condition_encoder(condition2)
        
        print(f"ç®€å•ç½‘ç»œè¾“å‡º1 mean: {torch.mean(out1):.6f}")
        print(f"ç®€å•ç½‘ç»œè¾“å‡º2 mean: {torch.mean(out2):.6f}")
        print(f"ç®€å•ç½‘ç»œè¾“å‡ºå·®å¼‚: {torch.mean(torch.abs(out1 - out2)):.6f}")
    
    return torch.mean(torch.abs(out1 - out2)).item() > 1e-4

if __name__ == "__main__":
    print("å¼€å§‹è°ƒè¯•conditionå½±å“...")
    
    # æµ‹è¯•ç®€å•ç½‘ç»œ
    simple_works = test_simple_condition_network()
    print(f"ç®€å•ç½‘ç»œæµ‹è¯•: {'âœ“ é€šè¿‡' if simple_works else 'âœ— å¤±è´¥'}")
    
    # æµ‹è¯•å®Œæ•´æ¨¡å‹
    full_works = test_condition_step_by_step()
    print(f"å®Œæ•´æ¨¡å‹æµ‹è¯•: {'âœ“ é€šè¿‡' if full_works else 'âœ— å¤±è´¥'}")
    
    if simple_works and full_works:
        print("\nğŸ‰ ConditionåŠŸèƒ½æ­£å¸¸å·¥ä½œï¼")
    elif simple_works and not full_works:
        print("\nâš  ç®€å•ç½‘ç»œå·¥ä½œï¼Œä½†å®Œæ•´æ¨¡å‹æœ‰é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
    else:
        print("\nâŒ åŸºç¡€ç½‘ç»œå°±æœ‰é—®é¢˜ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
