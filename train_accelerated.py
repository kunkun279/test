#!/usr/bin/env python3
"""
åŠ é€Ÿè®­ç»ƒè„šæœ¬ï¼šåŒ…å«å¤šç§è®­ç»ƒä¼˜åŒ–æŠ€å·§
"""

import argparse
import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import time

sys.path.append(os.getcwd())
from utils import create_logger, seed_set
from utils.script import *
from config import Config, update_config
from tensorboardX import SummaryWriter
from utils.training import Trainer
from utils.evaluation import compute_stats

class AcceleratedTrainer(Trainer):
    """åŠ é€Ÿè®­ç»ƒå™¨ï¼ŒåŒ…å«å¤šç§ä¼˜åŒ–æŠ€å·§"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = True
        self.scaler = GradScaler() if self.use_amp else None

        # ç´¯ç§¯æ¢¯åº¦
        self.accumulation_steps = 2  # æ¯2æ­¥æ›´æ–°ä¸€æ¬¡æ¢¯åº¦
        self.accumulated_loss = 0.0
        self.step_count = 0

        # æ—©åœæœºåˆ¶
        self.best_val_loss = float('inf')
        self.patience = 20
        self.patience_counter = 0

        # åŠ¨æ€æ‰¹é‡å¤§å°
        self.dynamic_batch_size = True
        self.max_batch_size = 256
        self.min_batch_size = 32

    def loop(self):
        """é‡å†™è®­ç»ƒå¾ªç¯ï¼Œæ·»åŠ æ—©åœæœºåˆ¶"""
        self.before_train()
        for self.iter in range(0, self.cfg.num_epoch):
            self.before_train_step()
            self.run_train_step()
            self.after_train_step()
            self.before_val_step()
            self.run_val_step()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ
            should_stop = self.after_val_step()
            if should_stop:
                self.logger.info("Early stopping triggered!")
                break
        
    def before_train(self):
        """æ”¹è¿›çš„è®­ç»ƒå‰è®¾ç½®"""
        # ä½¿ç”¨AdamWä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.cfg.lr,
            weight_decay=1e-4,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # ä½¿ç”¨OneCycleLRè¿›è¡Œæ›´æ¿€è¿›çš„å­¦ä¹ ç‡è°ƒåº¦
        self.lr_scheduler = optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=self.cfg.lr * 10,  # æœ€å¤§å­¦ä¹ ç‡æ˜¯åˆå§‹å­¦ä¹ ç‡çš„10å€
            total_steps=self.cfg.num_epoch,
            pct_start=0.1,  # 10%çš„æ—¶é—´ç”¨äºwarmup
            anneal_strategy='cos',
            div_factor=10.0,  # åˆå§‹å­¦ä¹ ç‡ = max_lr / div_factor
            final_div_factor=100.0  # æœ€ç»ˆå­¦ä¹ ç‡ = max_lr / final_div_factor
        )
        
        self.criterion = nn.MSELoss()
        
        # ç¼–è¯‘æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model)
                self.logger.info("Model compiled successfully!")
            except Exception as e:
                self.logger.info(f"Model compilation failed: {e}")
    
    def run_train_step(self):
        """æ”¹è¿›çš„è®­ç»ƒæ­¥éª¤"""
        
        for traj_np in self.generator_train:
            with torch.no_grad():
                # æ•°æ®é¢„å¤„ç†
                traj_np = traj_np[..., 1:, :].reshape([traj_np.shape[0], self.cfg.t_his + self.cfg.t_pred, -1])
                traj = tensor(traj_np, device=self.cfg.device, dtype=self.cfg.dtype)
                traj_pad = padding_traj(traj, self.cfg.padding, self.cfg.idx_pad, self.cfg.zero_index)
                traj_dct = torch.matmul(self.cfg.dct_m_all[:self.cfg.n_pre], traj)
                traj_dct_mod = torch.matmul(self.cfg.dct_m_all[:self.cfg.n_pre], traj_pad)
                if np.random.random() > self.cfg.mod_train:
                    traj_dct_mod = None

                # conditionå¤„ç†
                if self.condition is not None:
                    batch_size = traj.shape[0]
                    condition_size = self.condition.shape[0]
                    
                    if condition_size >= batch_size:
                        indices = torch.randperm(condition_size)[:batch_size]
                        condition_batch = self.condition[indices]
                    else:
                        indices = torch.arange(batch_size) % condition_size
                        condition_batch = self.condition[indices]
                else:
                    condition_batch = None

            # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
            if self.use_amp:
                with autocast():
                    t = self.diffusion.sample_timesteps(traj.shape[0]).to(self.cfg.device)
                    x_t, noise = self.diffusion.noise_motion(traj_dct, t)
                    predicted_noise = self.model(x_t, t, mod=traj_dct_mod, condition=condition_batch)
                    loss = self.criterion(predicted_noise, noise)
                    loss = loss / self.accumulation_steps  # å½’ä¸€åŒ–ç´¯ç§¯æŸå¤±
            else:
                t = self.diffusion.sample_timesteps(traj.shape[0]).to(self.cfg.device)
                x_t, noise = self.diffusion.noise_motion(traj_dct, t)
                predicted_noise = self.model(x_t, t, mod=traj_dct_mod, condition=condition_batch)
                loss = self.criterion(predicted_noise, noise)
                loss = loss / self.accumulation_steps

            # åå‘ä¼ æ’­
            if self.use_amp:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()

            self.accumulated_loss += loss.item()
            self.step_count += 1

            # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
            if self.step_count % self.accumulation_steps == 0:
                if self.use_amp:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                
                self.optimizer.zero_grad()

                # EMAæ›´æ–°
                if self.ema_setup and self.ema_setup[0]:
                    self.ema_setup[1].step_ema(self.ema_setup[2], self.model)

                # è®°å½•æŸå¤±
                self.train_losses.update(self.accumulated_loss)
                self.tb_logger.add_scalar('Loss/train', self.accumulated_loss, self.iter)
                
                self.accumulated_loss = 0.0

            del loss, traj, traj_dct, traj_dct_mod, traj_pad, traj_np

    def after_val_step(self):
        """æ”¹è¿›çš„éªŒè¯åå¤„ç†"""
        super().after_val_step()
        
        # æ—©åœæ£€æŸ¥
        current_val_loss = self.val_losses.avg
        if current_val_loss < self.best_val_loss:
            self.best_val_loss = current_val_loss
            self.patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(self.model.state_dict(), 
                      os.path.join(self.cfg.model_dir, f'best_model_epoch_{self.iter}.pt'))
            self.logger.info(f"New best validation loss: {self.best_val_loss:.6f}")
        else:
            self.patience_counter += 1
            if self.patience_counter >= self.patience:
                self.logger.info(f"Early stopping triggered after {self.patience} epochs without improvement")
                return True  # è§¦å‘æ—©åœ
        
        return False

    def after_train_step(self):
        """æ”¹è¿›çš„è®­ç»ƒåå¤„ç†"""
        # å­¦ä¹ ç‡è°ƒåº¦
        self.lr_scheduler.step()
        self.lrs.append(self.optimizer.param_groups[0]['lr'])
        
        self.logger.info(
            '====> Epoch: {} Time: {:.2f} Train Loss: {:.6f} lr: {:.6f}'.format(
                self.iter,
                time.time() - self.t_s,
                self.train_losses.avg,
                self.lrs[-1]))
        
        # åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°ï¼ˆå¯é€‰ï¼‰
        if self.dynamic_batch_size and self.iter > 0 and self.iter % 50 == 0:
            self.adjust_batch_size()
        
        # å…¶ä»–åŸæœ‰é€»è¾‘...
        if self.iter % self.cfg.save_gif_interval == 0:
            pose_gen = pose_generator(self.dataset['train'], self.model, self.diffusion, self.cfg, condition=self.condition, mode='gif')
            render_animation(self.dataset['train'].skeleton, pose_gen, ['HumanMAC'], self.cfg.t_his, ncol=4,
                           output=os.path.join(self.cfg.gif_dir, f'training_{self.iter}.gif'))

    def adjust_batch_size(self):
        """åŠ¨æ€è°ƒæ•´æ‰¹é‡å¤§å°"""
        try:
            # æ ¹æ®GPUå†…å­˜ä½¿ç”¨æƒ…å†µè°ƒæ•´æ‰¹é‡å¤§å°
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()
                if memory_used < 0.7 and self.cfg.batch_size < self.max_batch_size:
                    self.cfg.batch_size = min(self.cfg.batch_size + 16, self.max_batch_size)
                    self.logger.info(f"Increased batch size to {self.cfg.batch_size}")
                elif memory_used > 0.9 and self.cfg.batch_size > self.min_batch_size:
                    self.cfg.batch_size = max(self.cfg.batch_size - 16, self.min_batch_size)
                    self.logger.info(f"Decreased batch size to {self.cfg.batch_size}")
        except Exception as e:
            self.logger.info(f"Failed to adjust batch size: {e}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--cfg', default='h36m', help='h36m or humaneva')
    parser.add_argument('--mode', default='train', help='train / eval')
    parser.add_argument('--iter', type=int, default=0)
    parser.add_argument('--seed', type=int, default=0)
    parser.add_argument('--device', type=str,
                        default=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))
    parser.add_argument('--multimodal_threshold', type=float, default=0.5)
    parser.add_argument('--multimodal_th_high', type=float, default=0.1)
    parser.add_argument('--milestone', type=list, default=[75, 150, 225, 275, 350, 450])
    parser.add_argument('--gamma', type=float, default=0.9)
    parser.add_argument('--save_model_interval', type=int, default=10)
    parser.add_argument('--save_gif_interval', type=int, default=10)
    parser.add_argument('--save_metrics_interval', type=int, default=100)
    parser.add_argument('--ckpt', type=str, default='./checkpoints/h36m_ckpt.pt')
    parser.add_argument('--ema', type=bool, default=True)
    parser.add_argument('--vis_switch_num', type=int, default=10)
    parser.add_argument('--vis_col', type=int, default=5)
    parser.add_argument('--vis_row', type=int, default=3)
    args = parser.parse_args()

    # è®¾ç½®
    seed_set(args.seed)
    cfg = Config(f'{args.cfg}', test=(args.mode != 'train'))
    cfg = update_config(cfg, vars(args))
    dataset, dataset_multi_test = dataset_split(cfg)

    # åŠ è½½conditionæ•°æ®ï¼ˆè…¿éƒ¨è§’é€Ÿåº¦å’Œè§’åŠ é€Ÿåº¦ï¼‰
    from scipy import io
    import numpy as np

    data_path = 'data/train'
    # è·å–æ‰€æœ‰ .mat æ–‡ä»¶è·¯å¾„
    mat_files = [os.path.join(data_path, file) for file in os.listdir(data_path) if file.endswith('.mat')]

    # åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ç”¨äºå­˜å‚¨è¯»å–çš„çŸ©é˜µ
    all_data = []

    # éå†æ–‡ä»¶ï¼Œè¯»å–å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­
    for mat_file in mat_files:
        data = io.loadmat(mat_file)['x']  # å‡è®¾æ¯ä¸ªæ–‡ä»¶éƒ½æœ‰é”® 'x'
        all_data.append(data)

    # åˆå¹¶æ‰€æœ‰æ•°æ®ï¼Œå‡è®¾éœ€è¦æ²¿ç¬¬ä¸€ä¸ªç»´åº¦æ‹¼æ¥
    condition = np.concatenate(all_data, axis=0)

    # è·å–conditionçš„ç»´åº¦
    cond_dim = condition.shape
    print("Condition dimension:", cond_dim)
    print("Condition data type: è…¿éƒ¨è§’é€Ÿåº¦å’Œè§’åŠ é€Ÿåº¦")

    # å¯¹è…¿éƒ¨è§’é€Ÿåº¦å’Œè§’åŠ é€Ÿåº¦æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
    condition_mean = np.mean(condition, axis=0, keepdims=True)
    condition_std = np.std(condition, axis=0, keepdims=True)
    condition_std = np.where(condition_std == 0, 1.0, condition_std)  # é¿å…é™¤é›¶
    condition_normalized = (condition - condition_mean) / condition_std

    print(f"Condition statistics:")
    print(f"  Original range: [{np.min(condition):.4f}, {np.max(condition):.4f}]")
    print(f"  Normalized range: [{np.min(condition_normalized):.4f}, {np.max(condition_normalized):.4f}]")
    print(f"  Mean: {np.mean(condition_normalized, axis=0)}")
    print(f"  Std: {np.std(condition_normalized, axis=0)}")

    condition = torch.tensor(condition_normalized, dtype=torch.float32)

    # ä¿å­˜æ ‡å‡†åŒ–å‚æ•°ï¼Œç”¨äºåç»­çš„æ¨ç†
    condition_stats = {
        'mean': condition_mean,
        'std': condition_std
    }
    np.save('condition_stats.npy', condition_stats)

    # æ—¥å¿—
    tb_logger = SummaryWriter(cfg.tb_dir)
    logger = create_logger(os.path.join(cfg.log_dir, 'log.txt'))
    display_exp_setting(logger, cfg)

    # æ¨¡å‹
    model, diffusion = create_model_and_diffusion(cfg, condition)
    if torch.cuda.device_count() > 1:
        logger.info(f"Using {torch.cuda.device_count()} GPUs.")
        model = torch.nn.DataParallel(model)

    logger.info(">>> total params: {:.2f}M".format(
        sum(p.numel() for p in list(model.parameters())) / 1000000.0))

    if args.mode == 'train':
        multimodal_dict = get_multimodal_gt_full(logger, dataset_multi_test, args, cfg)
        condition = condition.clone().detach().to(cfg.device)
        
        # ä½¿ç”¨åŠ é€Ÿè®­ç»ƒå™¨
        trainer = AcceleratedTrainer(
            model=model,
            diffusion=diffusion,
            dataset=dataset,
            cfg=cfg,
            multimodal_dict=multimodal_dict,
            logger=logger,
            tb_logger=tb_logger,
            condition=condition)
        
        logger.info("ğŸš€ Starting accelerated training...")
        trainer.loop()

if __name__ == '__main__':
    main()
